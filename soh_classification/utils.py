import torch.nn as nn
import torch
import numpy as np
import random
import pandas as pd
import os
import joblib

class AnomalyProcessor:
    '''
    å¯¹æ•°æ®è¿›è¡ŒåŠ å™ªå£°ã€åŠ ç¼ºå¤±ã€åŠ å˜æ¢å¹²æ‰°
    '''
    def __init__(self, data_list=None):
        self.data_list = data_list
        self.key_name =[
            'å…‰å­¦æ¨¡å—ç”µå‹',
            'æ§åˆ¶æ¨¡å—ç”µæµ',
        ]
    def noise_adder(self, data_list, noise_level=0.15, noise_rate=0.3, **kwargs):
        """
        å¯¹æ•°æ®è¿›è¡ŒåŠ å™ªå£°å¹²æ‰°
        """
        #print('Adding Noise with noise level:', noise_level)
        # å¾—åˆ°æ•°æ®çš„å¹³å‡å¹…åº¦å€¼
        avg_values = {}
        for key in data_list[0].keys(): 
            data = [data[key] for data in data_list]
            avg_values[key] = noise_level * abs(sum(data)) / len(data)
        # ç”Ÿæˆå™ªå£°æ˜ å°„è¡¨
        key_noise_map = {}
        for key, avg_value in avg_values.items():
            key_noise_map[key] = avg_value * random.uniform(0.98, 1.02)
        # å¯¹æ•°æ®è¿›è¡ŒåŠ å™ªå£°
        for i in range(len(data_list)):
            available_keys = [key for key in data_list[i].keys() if key in key_noise_map]
            if not available_keys:
                continue
            if random.random() > noise_rate:
                continue
            num_keys_to_select = random.randint(1, 3)
            #selected_keys = random.sample(available_keys, num_keys_to_select)
            selected_keys = available_keys
            for key in selected_keys:
                data_list[i][key] += np.random.normal(0, key_noise_map[key])
        return data_list
    
    def missing_adder(self, data_list, missing_rate=1.0, **kwargs):
        """
        å¯¹æ•°æ®è¿›è¡ŒåŠ ç¼ºå¤±å¹²æ‰°
        """
        #print('Adding Missing with missing rate:', missing_rate)
        for data in data_list:
            for key in data:
                if random.random() < missing_rate:
                    data[key] = 0.0
        return data_list
    
    def data_transform(self, data_list, transform_rate=1.0, **kwargs):
        """
        å¯¹æ•°æ®è¿›è¡ŒåŠ å˜æ¢å¹²æ‰°
        """
        #print('Adding Transform with transform rate:', transform_rate)
        for data in data_list:
            for key in data:
                if random.random() < transform_rate and key in self.key_name:
                    if random.random() < 0.5:
                        data[key] *= random.uniform(0.8, 1.2)  # ç¼©æ”¾
                    else:
                        data[key] += random.uniform(-0.5, 0.5)  # å¹³ç§»
        return data_list

    def outlier_adder(self, data_list, outlier_rate=1.0, factor=2.0, **kwargs):
        """
        å¯¹æ•°æ®è¿›è¡ŒåŠ å¼‚å¸¸å€¼å¹²æ‰°
        """
        #print('Adding Outlier with outlier rate:', outlier_rate)
        for data in data_list:
            for key in data:
                if random.random() < outlier_rate:
                    data[key] *= random.choice([-factor, factor])  # å¼‚å¸¸å€¼
        return data_list
    
    def data_random_process(self, data_list, noise_level=0.15, rate=0.25, factor=5.0, **kwargs):
        """
        å¯¹æ•°æ®éšæœºè¿›è¡ŒåŠ å™ªå£°ã€åŠ ç¼ºå¤±ã€åŠ å˜æ¢å¹²æ‰°
        """
        for data in data_list:
            if random.random() > rate:
                continue
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ•°æ®å¢å¼ºå‡½æ•°
            augment_func = random.choice([
                self.noise_adder,
                #self.missing_adder,
                self.data_transform,
                self.outlier_adder
            ])
            # åº”ç”¨å¢å¼ºå‡½æ•°
            data = augment_func([data], noise_level=noise_level, factor=factor)[0]
        return data_list
    def key_mapper(self, key_map, **kwargs):
        """
        å¯¹æ•°æ®è¿›è¡Œæ˜ å°„,å®ç°é€‰å–ç‰¹å®šçš„æ¨¡å—æ•…éšœ"""
        pass
    
class SOHDetector:
    '''
    è‡ªé€‚åº”é˜ˆå€¼çš„SOHæ£€æµ‹å™¨
    :param soh_predictor: ç”¨äºé¢„æµ‹SOHçš„æ¨¡å‹
    :param normal_features: æ­£å¸¸å·¥ä½œæ—¶çš„ç‰¹å¾æ•°æ®
    :param device: è®¾å¤‡
    :param threshold: åˆå§‹é˜ˆå€¼
    :param auto_threshold: æ˜¯å¦å¯ç”¨è‡ªé€‚åº”é˜ˆå€¼
    :param pid_params: PIDæ§åˆ¶å™¨å‚æ•°
    '''
    def __init__(self, soh_predictor, cls_model, normal_features, device, threshold=0.8, auto_threshold=False, pid_params=None, normalize=False, sclar_soh_path=None, sclar_cls_path=None, filter=False, print_log=False):
        self.soh_predictor = soh_predictor
        self.cls_model = cls_model
        self.loss_fn = nn.MSELoss(reduction='none')
        self.normal_features = normal_features
        self.device = device
        self.threshold = threshold
        self.auto_threshold = auto_threshold
        self.threshold_alpha = 0.95
        self.normal_loss = None
        self.filter = filter
        self.count = np.zeros(4+3+1+1)
        # ç”¨äºå­˜å‚¨å†å²æ ·æœ¬ï¼Œæ›´æ–°é˜ˆå€¼
        self.remembered_features = []
        self.remembered_soh = []
        self.remembered_num = 32
        # PID æ§åˆ¶å™¨å‚æ•°
        self.pid_params = pid_params if pid_params else {'Kp': 0.01, 'Ki': 0.000015, 'Kd': 0.2}
        self.prev_error = 0
        self.integral = 0
        # æ ‡å‡†åŒ–å‚æ•°
        self.normalize = normalize
        if self.normalize:
            self.scaler_soh, self.indices_soh = joblib.load(sclar_soh_path)
            self.scaler_cls, self.indices_cls = joblib.load(sclar_cls_path)
        # æ—¥å¿—ä¿¡æ¯
        self.log_info = []
        self.print_log = print_log
        # è®¡ç®—åˆå§‹æ ·æœ¬åˆ†å¸ƒ
        self.calculate_normal_loss()

    def detect_soh(self, features, normal_loss=None, alpha=0.1, key_map=None, output=True):
        """
        æ£€æµ‹å¼‚å¸¸
        """
        if normal_loss is None:
            normal_loss = self.normal_loss
        self.soh_predictor.eval()
        if self.normalize:
            data_soh, data_cls = self.Normalize(features)
        with torch.no_grad():   
            inputs = torch.tensor(data_soh, dtype=torch.float32).to(self.device)
            outputs = self.soh_predictor(inputs)
            loss = self.loss_fn(outputs, inputs).to('cpu').numpy()
            mean_loss = np.mean(loss, axis=0)
            # å¯¹æŸä¸ªæ¨¡å—è®¡ç®—SOHï¼Œæœ€ç»ˆSOHå–æœ€å°å€¼      
            normalized_loss = np.zeros_like(mean_loss)
            soh = np.zeros_like(mean_loss)
            for i in range(len(mean_loss)):
                normalized_loss[i] = abs(mean_loss[i] - normal_loss[0][i]) / (normal_loss[1][i] + 1e-6)
                soh[i] = 100 * np.exp(-normalized_loss[i] * alpha)

            # å¾—åˆ°SOHä»¥åŠå¯¹åº”ç´¢å¼•
            mean_soh = np.mean(soh)
            if self.filter:
                mean_soh = self.smooth(mean_soh)
            # å¦‚æœåªè®¡ç®—ç»“æœç”¨äºå…¶ä»–æ–¹æ³•ï¼Œä¸è¾“å‡º
            if output == False:
                return mean_soh, self.threshold, soh
            
            # è¾“å‡ºé¢„è­¦ä¿¡æ¯
            warning_type = self.warning(mean_soh, data_soh, data_cls, key_map=key_map)

            # æ›´æ–°é˜ˆå€¼
            if self.auto_threshold:
                self.update_threshold(features, mean_soh, mean_loss)
        return mean_soh, self.threshold, warning_type
    
    def update_threshold(self, new_features=None, new_soh=None, mean_loss=None, new_threshold=None, update_normal_loss=False):
        """
        è‡ªé€‚åº”æ›´æ–°é˜ˆå€¼
        """
        if new_threshold is not None:
            self.threshold = new_threshold
            return
        # è®¡ç®—æ–°çš„é˜ˆå€¼(PID)
        error = 0.01 * (new_soh) * self.threshold_alpha - self.threshold
        pid_output = self.pid_controller(error)
        #th_limit = np.mean(self.remembered_soh)*self.threshold_alpha*0.01
        #th_limit = self.threshold_alpha
        self.threshold = np.clip(self.threshold + pid_output, 0, 1)
        
        if new_soh is None or new_features is None or mean_loss is None:
            return
        # æ ¹æ®æ ·æœ¬é‡æ„è¯¯å·®ç¡®å®šæ˜¯å¦è¦ä¿ç•™æ ·æœ¬
        z_score = (mean_loss - self.normal_loss[0]) / self.normal_loss[1]
        if np.max(z_score) > 3: # 3 sigmaåŸåˆ™ï¼Œæœ‰ä¸€ä¸ªç»´åº¦å¼‚å¸¸å°±ä¸æ›´æ–°
            return 

        # æ›´æ–°å­˜å‚¨æ ·æœ¬(åªä¿ç•™å¥åº·æ ·æœ¬)
        if len(self.remembered_features) < self.remembered_num and new_soh > self.threshold:
            self.remembered_features.append(new_features)
        elif len(self.remembered_features) >= self.remembered_num and new_soh > self.threshold:
            self.remembered_features.pop(0)
            self.remembered_features.append(new_features)

        if update_normal_loss:
            features = np.mean(self.remembered_features, axis=0)
            self.calculate_normal_loss(features)
    

    def calculate_normal_loss(self, normal_features=None):
        """
        è®¡ç®—æ­£å¸¸æ ·æœ¬çš„å¹³å‡æŸå¤±å’Œæ ‡å‡†å·®
        """
        if normal_features is None:
            normal_features = self.normal_features
        
        if self.normalize:
            normal_features, _ = self.Normalize(normal_features)

        self.soh_predictor.eval()
        with torch.no_grad():
            inputs = torch.tensor(normal_features, dtype=torch.float32).to(self.device)
            outputs = self.soh_predictor(inputs)
            loss = self.loss_fn(outputs, inputs).to('cpu').numpy()
            self.normal_loss = [np.mean(loss, axis=0), np.std(loss, axis=0)]
        #print(f'ğŸ¤“Calculating Normal loss: {self.normal_loss}')

    def pid_controller(self, error):
        """
        PID æ§åˆ¶å™¨è®¡ç®—
        """
        # è®¡ç®—æ¯”ä¾‹ã€ç§¯åˆ†å’Œå¾®åˆ†é¡¹
        self.integral += error
        derivative = error - self.prev_error
        output = (self.pid_params['Kp'] * error + 
                  self.pid_params['Ki'] * self.integral + 
                  self.pid_params['Kd'] * derivative)
        self.prev_error = error
        return output
    
    def warning(self, soh, data_soh, data_cls, key_map=None):
        '''
        æ ¹æ®å†å²sohå€¼ç»™å‡ºé¢„è­¦
        return:
            0: æ— å¼‚å¸¸
            1: æ¨¡å—å¼‚å¸¸
            2: å¥åº·åº¦è¿ç»­ä¸‹é™
            3: å¥åº·åº¦é•¿æœŸæ¿’ä¸´é˜ˆå€¼
        '''
        from datetime import datetime
        # æ›´æ–°sohåˆ—è¡¨
        if len(self.remembered_soh) < self.remembered_num:
            self.remembered_soh.append(soh)
        elif len(self.remembered_soh) >= self.remembered_num:
            self.remembered_soh.pop(0)
            self.remembered_soh.append(soh)
        #### 1. æ¨¡å—å¼‚å¸¸é¢„è­¦
        if soh < (self.threshold * 100):
            # è·å–ç‰¹å¾é‡è¦æ€§
            inputs = torch.tensor(data_cls, dtype=torch.float32).to(self.device)
            logit = self.cls_model(inputs)
            probs = torch.softmax(logit, dim=1)
            confidence, pred_module = torch.max(probs, dim=1)
            # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„æ¨¡å—
            min_index = self.analyze_anomaly_module(pred_module[0].item())
            if min_index != 'Normal':
                log = f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]}>>å¥åº·åº¦ä½äºé˜ˆå€¼! å¥åº·åº¦:{soh:.2f} é˜ˆå€¼:{self.threshold * 100:.2f} å¯èƒ½æ•…éšœæ¨¡å—: {min_index}'
                self.log_info.append(log)
                if self.print_log:
                    print(log)
            else:
                log = f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]}>>å¥åº·åº¦ä½äºé˜ˆå€¼! å¥åº·åº¦:{soh:.2f} é˜ˆå€¼:{self.threshold * 100:.2f}'
                self.log_info.append(log)
                if self.print_log:
                    print(log)
                    #print(f'Loss: {mean_loss[min_index]}')
            return 1
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œä¸è¿›è¡Œå…¶ä»–é¢„è­¦
        if len(self.remembered_soh) < self.remembered_num:
            return 0
        
        #### 2. å¥åº·åº¦è¿ç»­ä¸‹é™é¢„è­¦
        # ç”¨ä¸€é˜¶çº¿æ€§å›å½’åˆ¤æ–­
        calc_soh = self.remembered_soh
        x = np.arange(len(calc_soh))
        y = np.array(calc_soh)
        A = np.vstack([x, np.ones(len(x))]).T
        slope, _ = np.linalg.lstsq(A, y, rcond=None)[0] # slope <= -0.1
        # ç”¨ä¸‹é™æ ·æœ¬æ¯”ä¾‹åˆ¤æ–­
        # soh_list = self.remembered_soh
        # diffs = [soh_list[i+1] - soh_list[i] for i in range(len(soh_list) - 1)]
        # drop_count = sum(d < 0 for d in diffs)
        # ratio = drop_count / len(diffs)
        if self.remembered_soh[-1] + self.remembered_num // 2 * slope < self.threshold * 100:
            log = f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]}>>å¥åº·åº¦æŒç»­ä¸‹é™! å¥åº·åº¦:{soh:.2f} é˜ˆå€¼:{self.threshold * 100:.2f}'
            self.log_info.append(log)
            if self.print_log:
                print(log)
            return 2
        #### 3. å¥åº·åº¦é•¿æœŸæ¿’ä¸´é˜ˆå€¼é¢„è­¦
        if all(soh - self.threshold * 100 < 3 for soh in self.remembered_soh):
            log = f'{datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]}>>å¥åº·åº¦é•¿æœŸæ¿’ä¸´é˜ˆå€¼! å¥åº·åº¦:{soh:.2f} é˜ˆå€¼:{self.threshold * 100:.2f}'
            self.log_info.append(log)
            if self.print_log:
                print(log)
            return 3
        return 0
    def analyze_anomaly_module(self, logit):
        """
        åˆ†æå¼‚å¸¸æ¨¡å—
        """
        # 4ä¸ªèˆµæœº 3ä¸ªæƒ¯ç»„ 1ä¸ªç”µæº 1ä¸ªåŒ—æ–—
        module = {
            0: 'Normal',
            1: 'èˆµæœº1',
            2: 'èˆµæœº2',
            3: 'èˆµæœº3',
            4: 'èˆµæœº4',
            5: 'æƒ¯ç»„Xè½´',
            6: 'æƒ¯ç»„Yè½´',
            7: 'æƒ¯ç»„Zè½´',
            8: 'ç”µæºæ¨¡å—',
            9: 'åŒ—æ–—æ¨¡å—'
        }
        if logit > 0:
            self.count[logit-1] += 1
        return module[logit]
    
    def Normalize(self, data):
        normalized_soh = data.copy()
        normalized_cls = data.copy()
        for scaler, i in zip(self.scaler_soh, self.indices_soh):
            normalized_soh[:, i] = scaler.transform(data[:, i].reshape(-1, 1)).flatten()
        for scaler, i in zip(self.scaler_cls, self.indices_cls):
            normalized_cls[:, i] = scaler.transform(data[:, i].reshape(-1, 1)).flatten()
        return normalized_soh, normalized_cls
    
    def smooth(self, soh, window_size=4, sigma=1.2):
        """
        å¯¹sohè¿›è¡Œå¹³æ»‘å¤„ç†
        """
        # å¹³æ»‘æ»¤æ³¢
        if len(self.remembered_soh) >= window_size:
            # æå–çª—å£æ•°æ®
            window_data = np.array(self.remembered_soh[-(window_size-1):])
            # åŠ å…¥æ–°çš„sohæ•°æ®
            window_data = np.append(window_data, soh)
            # ç”Ÿæˆæƒé‡
            center = (window_size - 1) // 2
            x = np.arange(window_size) - center
            weights = np.exp(-0.5 * (x / sigma) ** 2)
            weights /= weights.sum()  # å½’ä¸€åŒ–
            
            # åŠ æƒå¹³å‡
            smoothed_soh = np.sum(window_data * weights)
        else:
            smoothed_soh = soh  # æ•°æ®ä¸è¶³æ—¶ç›´æ¥ä½¿ç”¨å½“å‰å€¼
        return smoothed_soh


class SOHPredictor2(nn.Module):
    def __init__(self, input_dim):
        super(SOHPredictor, self).__init__()
        # ç¼–ç å™¨éƒ¨åˆ†
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
        )
        self.self_attn = nn.MultiheadAttention(embed_dim=128, num_heads=4, dropout=0.3, batch_first=True)
        self.attn_layer_norm = nn.LayerNorm(128)
        # è§£ç å™¨éƒ¨åˆ†ï¼Œç”¨äºé‡æ„è¾“å…¥
        self.decoder = nn.Sequential(
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim)
        )
        # æ®‹å·®é“¾æ¥
        self.skip = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        encoded = self.encoder(x)
        # è‡ªæ³¨æ„åŠ› (æ·»åŠ æ®‹å·®è¿æ¥å’ŒLayerNorm)
        attn_output, _ = self.self_attn(encoded, encoded, encoded)
        encoded = self.attn_layer_norm(encoded + attn_output)
        decoded = self.decoder(encoded)
        return decoded + self.skip(x)  # æ®‹å·®è¿æ¥

class SOHPredictor(nn.Module):
    def __init__(self, input_dim):
        super(SOHPredictor, self).__init__()
        # ç¼–ç å™¨éƒ¨åˆ†
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LayerNorm(512),  # æ·»åŠ å½’ä¸€åŒ–
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(512, 256),
            nn.LayerNorm(256),  # æ·»åŠ å½’ä¸€åŒ–
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(256, 128),
            nn.LayerNorm(128),  # æ·»åŠ å½’ä¸€åŒ–
            nn.ReLU(),
        )
        # è§£ç å™¨éƒ¨åˆ†ï¼Œç”¨äºé‡æ„è¾“å…¥
        self.decoder = nn.Sequential(
            nn.Linear(128, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(256, 512),
            nn.LayerNorm(512),
            nn.ReLU(),

            nn.Linear(512, input_dim)
        )
        # æ®‹å·®é“¾æ¥
        self.skip = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded + self.skip(x)  # æ®‹å·®è¿æ¥

class CustomLoss(nn.Module):
    def __init__(self, alpha=0.5):
        super().__init__()
        self.alpha = alpha  # æ§åˆ¶ç‰¹å¾é—´å¹³è¡¡çš„æƒé‡
        
    def forward(self, outputs, inputs):
        # åŸºç¡€MSEæŸå¤±
        mse_loss = torch.abs(torch.mean((outputs - inputs)**2, dim=0))  # æŒ‰ç‰¹å¾è®¡ç®—
        
        # æ·»åŠ ç‰¹å¾é—´å¹³è¡¡é¡¹ï¼šæœ€å°åŒ–å„ç‰¹å¾æŸå¤±çš„æ ‡å‡†å·®
        std_loss = torch.std(mse_loss)
        total_loss = torch.mean(mse_loss) + self.alpha * std_loss
        return total_loss

class ModuleAwareLoss(nn.Module):
    def __init__(self, device, alpha=0.5, module_ranges=None):
        super().__init__()
        self.alpha = alpha
        # å®šä¹‰æ¨¡å—åˆ’åˆ† (ä¸SOHDetectorä¸­ä¸€è‡´)
        self.module_ranges = module_ranges if module_ranges else [
            (0, 6),    # Motor1
            (6, 12),   # Motor2
            (12, 18),  # Motor3
            (18, 24),  # Motor4
            [24, 27, 30],  # EulerX (Accelx, AngAcx, Eulerx)
            [25, 28, 31],  # EulerY
            [26, 29, 32],  # EulerZ
            (33, 37),  # Power
            (37, 40)   # Beidou
        ]
        # lossåŠ æƒ
        motor = 3.0
        euler = 1.0
        power = 1.5
        beidou = 1.0
        self.module_weights = torch.tensor([motor, motor, motor, motor,   # ç”µæœº
                              euler, euler, euler,         # æƒ¯ç»„
                              power,                   # ç”µæº
                              beidou]                  # åŒ—æ–—
                             ).to(device)
    def forward(self, outputs, inputs):
        # åŸºç¡€MSEæŸå¤±
        mse_loss = torch.mean((outputs - inputs)**2, dim=0)  # æŒ‰ç‰¹å¾è®¡ç®—

        # è®¡ç®—æ¯ä¸ªæ¨¡å—çš„æŸå¤±
        module_losses = []
        weighted_losses = []
        for i, module in enumerate(self.module_ranges):
            if isinstance(module, tuple):
                # è¿ç»­èŒƒå›´
                indices = list(range(module[0], module[1]))
            else:
                # ç¦»æ•£ç´¢å¼•
                indices = module
                
            module_loss = torch.mean(mse_loss[indices])
            weighted_loss = module_loss * self.module_weights[i]
            module_losses.append(module_loss)
            weighted_losses.append(weighted_loss)
        
        # æ€»æŸå¤± = æ¨¡å—æŸå¤±çš„å¹³å‡ + æ¨¡å—é—´å¹³è¡¡é¡¹
        total_module_loss = torch.mean(torch.stack(weighted_losses))
        balance_loss = torch.std(torch.stack(module_losses))  # å¹³è¡¡å„æ¨¡å—æŸå¤±
        #mean_loss = torch.mean(mse_loss)
        return total_module_loss + self.alpha * balance_loss

def GPS_relative(data_list):
    '''
    æ•°æ®ç»´åº¦ä¸º40ï¼Œæœ€åä¸‰ä¸ªç»´åº¦ä¸ºGPSåæ ‡ï¼Œå°†ç»å¯¹åæ ‡è½¬åŒ–ä¸ºç›¸å¯¹åæ ‡
    åŒæ—¶è½¬åŒ–å€’æ•°ç¬¬å››ä¸ªç»´åº¦ä¸ºç”µé‡ï¼Œå°†ç»å¯¹ç”µé‡è½¬åŒ–ä¸ºç›¸å¯¹ç”µé‡
    param: data_list: æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«40ä¸ªç‰¹å¾
    return: ç›¸å¯¹åæ ‡æ•°æ®åˆ—è¡¨
    '''
    # å°† data_list è½¬ä¸ºäºŒç»´æ•°ç»„
    data_array = np.array([list(entry.values()) for entry in data_list])
    gps_data = data_array[:, -3:]            # æå– GPSï¼ˆä¸‰ç»´ï¼‰
    features_wo_gps = data_array[:, :-3]     # å»é™¤ GPS çš„å…¶ä»–ç‰¹å¾

    # è®¡ç®—å‚è€ƒ GPS ç‚¹
    relative_gps = np.zeros_like(gps_data)
    relative_gps[1:, :] = gps_data[1:, :] - gps_data[:-1, :]  # å·®åˆ†å¤„ç†
    relative_gps[0, :] = 0

    # åˆå¹¶ç‰¹å¾å’Œå¤„ç†åçš„ GPS
    transformed_array = np.hstack([features_wo_gps, relative_gps])

    # é‡æ–°æ„å»ºå›åŸå§‹çš„ list[dict] å½¢å¼
    keys = list(data_list[0].keys())
    transformed_data_list = []
    for row in transformed_array:
        entry = {k: float(v) for k, v in zip(keys, row)}
        transformed_data_list.append(entry)

    return transformed_data_list

def Euler_relative(data_list):
    '''
    æ•°æ®ç»´åº¦ä¸º40ï¼Œå€’æ•°7ã€8ã€9ä¸ºå§¿æ€è§’ï¼Œå°†ç»å¯¹è§’åº¦è½¬åŒ–ä¸ºç›¸å¯¹è§’åº¦
    param: data_list: æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«40ä¸ªç‰¹å¾
    '''
    # å°† data_list è½¬ä¸ºäºŒç»´æ•°ç»„
    data_array = np.array([list(entry.values()) for entry in data_list])
    gps_data = data_array[:, -10:-7]           
    features_wo_gps = np.hstack([data_array[:, :-10], data_array[:, -7:]])

    # è®¡ç®—å‚è€ƒEuler
    relative_gps = np.zeros_like(gps_data)
    relative_gps[1:, :] = gps_data[1:, :] - gps_data[:-1, :]  # å·®åˆ†å¤„ç†
    relative_gps[0, :] = 0

    # åˆå¹¶ç‰¹å¾å’Œå¤„ç†åçš„Euler
    transformed_array = np.hstack([features_wo_gps[:, :-7], relative_gps, features_wo_gps[:, -7:]])

    # é‡æ–°æ„å»ºå›åŸå§‹çš„ list[dict] å½¢å¼
    keys = list(data_list[0].keys())
    transformed_data_list = []
    for row in transformed_array:
        entry = {k: float(v) for k, v in zip(keys, row)}
        transformed_data_list.append(entry)

    return transformed_data_list

def Battery_relative(data_list):
    # å°† data_list è½¬ä¸ºäºŒç»´æ•°ç»„
    data_array = np.array([list(entry.values()) for entry in data_list])
    gps_data = np.vstack([data_array[:, 3], data_array[:, 9], data_array[:, 15], data_array[:, 21], data_array[:, 36]]).T
    features_wo_gps = np.hstack([data_array[:, :3], data_array[:, 4:9], data_array[:, 10:15], data_array[:, 16:21], data_array[:, 22:36], data_array[:, 37:]])

    # è®¡ç®—å‚è€ƒ GPS ç‚¹
    relative_gps = np.zeros_like(gps_data)
    relative_gps[1:, :] = gps_data[1:, :] - gps_data[:-1, :]  # å·®åˆ†å¤„ç†
    relative_gps[0, :] = 0

    # åˆå¹¶ç‰¹å¾å’Œå¤„ç†åçš„ GPS
    transformed_array = np.hstack([features_wo_gps[:, :3], relative_gps[:, 0:1], 
                                   features_wo_gps[:, 3:8], relative_gps[:, 1:2],
                                   features_wo_gps[:, 8:13], relative_gps[:, 2:3],
                                   features_wo_gps[:, 13:18], relative_gps[:, 3:4],
                                   features_wo_gps[:, 18:32], relative_gps[:, 4:5],
                                   features_wo_gps[:, 32:]])

    # é‡æ–°æ„å»ºå›åŸå§‹çš„ list[dict] å½¢å¼
    keys = list(data_list[0].keys())
    transformed_data_list = []
    for row in transformed_array:
        entry = {k: float(v) for k, v in zip(keys, row)}
        transformed_data_list.append(entry)

    return transformed_data_list

def Motor_rolling_window_features(data_list, window_size=3):
    '''
    å¯¹èˆµæœºçš„æ•°æ®åšæ»‘åŠ¨çª—å£å¤„ç†ï¼ŒæŒ‰æ—¶é—´ç»´åº¦è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®
    '''
    # å°† data_list è½¬ä¸ºäºŒç»´æ•°ç»„
    data_array = np.array([list(entry.values()) for entry in data_list])
    
    # è·å– GPS æ•°æ®å’Œå…¶ä»–ç‰¹å¾æ•°æ®
    gps_data = data_array[:, 0:24]  # å‡è®¾å‰ 24 åˆ—æ˜¯ GPS æ•°æ®
    features_wo_gps = data_array[:, 24:]  # å…¶ä½™ä¸ºç‰¹å¾æ•°æ®
    
    # è®¡ç®—æ¯ä¸ªç‰¹å¾åœ¨æ»‘åŠ¨çª—å£å†…çš„å‡å€¼å’Œæ–¹å·®ï¼ˆæŒ‰æ—¶é—´ç»´åº¦ï¼‰
    rolling_means = np.array([
        np.array([np.mean(gps_data[i:i+window_size, j]) for i in range(gps_data.shape[0] - window_size + 1)])
        for j in range(gps_data.shape[1])
    ]).T  # æ»‘åŠ¨çª—å£å‡å€¼

    rolling_means = np.vstack([np.tile(rolling_means[0], (window_size-1, 1)), rolling_means])

    rolling_stds = np.array([
        np.array([np.std(gps_data[i:i+window_size, j]) for i in range(gps_data.shape[0] - window_size + 1)])
        for j in range(gps_data.shape[1])
    ]).T  # æ»‘åŠ¨çª—å£æ–¹å·®
    rolling_stds = np.vstack([np.tile(rolling_stds[0], (window_size-1, 1)), rolling_stds]) 
    # å°†å‡å€¼å’Œæ–¹å·®ç‰¹å¾æ‹¼æ¥èµ·æ¥

    transformed_features = np.hstack([rolling_means, rolling_stds])
    
    # å°†å¤„ç†åçš„æ•°æ®ä¸ GPS æ•°æ®åˆå¹¶
    transformed_array = np.hstack([rolling_stds, features_wo_gps])  # å¤„ç†åçš„æ•°æ®éœ€è¦åˆ é™¤å‰é¢çš„ NAs

    # é‡æ–°æ„å»ºå›åŸå§‹çš„ list[dict] å½¢å¼
    keys = list(data_list[0].keys())
    transformed_data_list = []
    
    for row in transformed_array:
        entry = {k: float(v) for k, v in zip(keys, row)}
        transformed_data_list.append(entry)

    return transformed_data_list

class ClassificationModel(nn.Module):
    def __init__(self, input_dim, num_modules):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.LayerNorm(512),  # æ·»åŠ å½’ä¸€åŒ–
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),

            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),

            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
        )
        
        # æ¨¡å—å¼‚å¸¸åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(64, 32),
            nn.LayerNorm(32),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(32, num_modules + 1)  # +1 for normal class
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        module_logits = self.classifier(encoded)
        return module_logits
    
def noise_adder(data_list, add_noise=True, noise_rate=0.4):
    """"
    å¯¹æ•°æ®æ·»åŠ å™ªå£°(æ•°æ®å¢å¼º)
    """
    if not add_noise:
        return data_list
    anomaly_processor = AnomalyProcessor()
    #bad_datas = anomaly_processor.noise_adder(data_list, noise_level=0.15, noise_rate=0.4) 
    # å°†data_listå’Œbad_datasçš„éƒ¨åˆ†æ ·æœ¬éšæœºæ··åˆå¾—åˆ°è®­ç»ƒæ ·æœ¬
    bad_data2 = load_data('./dataset/ç”µæºæ•…éšœ.txt', add_noise=False)
    bad_data3 = load_data('./dataset/åŒ—æ–—æ•…éšœ.txt', add_noise=False)
    bad_data4 = load_data('./dataset/èˆµæœº1æ•…éšœ.txt', add_noise=False)
    # ä»bad_dataä¸­éšæœºé€‰æ‹©Nä¸ªæ ·æœ¬
    N = len(data_list) // 20
    bad_data2 = random.sample(bad_data2, N)
    bad_data3 = random.sample(bad_data3, N)
    bad_data4 = random.sample(bad_data4, N)
    bad_datas1 = bad_data2 + bad_data3 + bad_data4
    bad_datas1 = anomaly_processor.data_random_process(data_list, noise_level=0.05, rate=1.0, factor=2)
    #bad_datas1 = random.sample(bad_datas1, len(data_list) // 10)
    train_datas = data_list + bad_datas1
    random.shuffle(train_datas)
    return train_datas

def load_data(data_path, add_noise=False):
    """
    åŠ è½½æ•°æ®é›†
    """
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Data file not found: {data_path}")
    columns = [
        'Motor1', 'Motor2', 'Motor3', 'Motor4', 'Motor5', 'Motor6', 'Motor7', 'Motor8',
        'Motor9', 'Motor10', 'Motor11', 'Motor12', 'Motor13', 'Motor14', 'Motor15', 'Motor16',
        'Motor17', 'Motor18', 'Motor19', 'Motor20', 'Motor21', 'Motor22', 'Motor23', 'Motor24',
        'Accelx', 'Accely', 'Accelz', 'AngAcx', 'AngAcy', 'AngAcz', 'Eulerx', 'Eulery', 'Eulerz',
        'Voltage', 'Current', 'Power', 'Battery',
        'GPS_longitude', 'GPS_latitude', 'GPS_altitude'
    ]
    data_pd = pd.read_csv(data_path, header=None, names=columns)
    # å°†DataFrameè½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    data = data_pd.to_dict(orient='records')
    # æ”¹å˜GPSåæ ‡
    data = GPS_relative(data)
    # æ”¹å˜Eulerè§’
    data = Euler_relative(data)
    # æ”¹å˜ç”µé‡
    data = Battery_relative(data)
    # èˆµæœºæ•°æ®åŠ çª—ç‰¹å¾æå–
    data = Motor_rolling_window_features(data, window_size=20)
    # å¼‚å¸¸æ³¨å…¥
    data_list = noise_adder(data, add_noise=add_noise)
    return data_list

def compress_logs(logs: list[str]) -> dict:
    """
    å°†åˆ—è¡¨è½¬åŒ–ä¸ºå¤§æ¨¡å‹è¾“å…¥çš„prompt
    """
    from datetime import datetime
    from collections import defaultdict
    import re
    def parse_log_time(log_str: str):
        # ä»æ—¥å¿—ä¸­æå–æ—¶é—´æˆ³å­—ç¬¦ä¸²å¹¶è½¬ä¸º datetime
        time_str = log_str.split('>', 1)[0]
        return time_str
    
    summary = {
        "å¥åº·åº¦æŒç»­ä¸‹é™": {
            "count": 0, "times": [], "soh_values": []
        },
        "å¥åº·åº¦ä½äºé˜ˆå€¼": {
            "count": 0, "times": [], "soh_values": [], "modules": defaultdict(int)
        },
        "å¥åº·åº¦æ¿’ä¸´é˜ˆå€¼": {
            "count": 0, "times": [], "soh_values": []
        }
    }
    for log in logs:
        if "å¥åº·åº¦æŒç»­ä¸‹é™" in log:
            summary["å¥åº·åº¦æŒç»­ä¸‹é™"]["count"] += 1
            summary["å¥åº·åº¦æŒç»­ä¸‹é™"]["times"].append(parse_log_time(log))
            soh_match = re.search(r'å¥åº·åº¦:(\d+\.?\d*)', log)
            if soh_match:
                summary["å¥åº·åº¦æŒç»­ä¸‹é™"]["soh_values"].append(float(soh_match.group(1)))

        elif "å¥åº·åº¦ä½äºé˜ˆå€¼" in log:
            summary["å¥åº·åº¦ä½äºé˜ˆå€¼"]["count"] += 1
            summary["å¥åº·åº¦ä½äºé˜ˆå€¼"]["times"].append(parse_log_time(log))
            soh_match = re.search(r'å¥åº·åº¦:(\d+\.?\d*)', log)
            if soh_match:
                summary["å¥åº·åº¦ä½äºé˜ˆå€¼"]["soh_values"].append(float(soh_match.group(1)))
            module_match = re.search(r'å¯èƒ½æ•…éšœæ¨¡å—:\s*(\S+)', log)
            if module_match:
                module = module_match.group(1)
                summary["å¥åº·åº¦ä½äºé˜ˆå€¼"]["modules"][module] += 1

        elif "å¥åº·åº¦é•¿æœŸæ¿’ä¸´é˜ˆå€¼" in log:
            summary["å¥åº·åº¦æ¿’ä¸´é˜ˆå€¼"]["count"] += 1
            summary["å¥åº·åº¦æ¿’ä¸´é˜ˆå€¼"]["times"].append(parse_log_time(log))
            soh_match = re.search(r'å¥åº·åº¦:(\d+\.?\d*)', log)
            if soh_match:
                summary["å¥åº·åº¦æ¿’ä¸´é˜ˆå€¼"]["soh_values"].append(float(soh_match.group(1)))

    # ç²¾ç‚¼è¾“å‡ºæ ¼å¼
    result = {}
    for key, value in summary.items():
        if value["count"] == 0:
            continue

        times = value["times"]
        result[key] = {
            "æ¬¡æ•°": value["count"],
            "æ—¶é—´èŒƒå›´": f"{min(times)} ~ {max(times)}" if times else "æ— è®°å½•",
            "SOHèŒƒå›´": f"{min(value['soh_values']):.2f} ~ {max(value['soh_values']):.2f}" if value["soh_values"] else "æœªçŸ¥",
        }

        if "modules" in value:
            result[key]["æ¨¡å—é¢‘æ¬¡"] = dict(sorted(value["modules"].items(), key=lambda x: -x[1]))
    #print(summary)
    print(result)
    prompt = f"""
    ã€æ—¥å¿—ä¿¡æ¯ä¸º Python å­—å…¸ï¼Œå­—æ®µè¯´æ˜å¦‚ä¸‹ã€‘ï¼š
    - å¥åº·åº¦æŒç»­ä¸‹é™: {{'æ¬¡æ•°': int, 'æ—¶é—´èŒƒå›´': str, 'SOHèŒƒå›´': str}}ï¼Œè¡¨æ˜è®¾å¤‡å¥åº·åº¦æŒç»­é™ä½
    - å¥åº·åº¦ä½äºé˜ˆå€¼: {{'æ¬¡æ•°': int, 'æ—¶é—´èŒƒå›´': str, 'SOHèŒƒå›´': str, 'æ¨¡å—é¢‘æ¬¡': dict}}ï¼Œè¡¨æ˜è®¾å¤‡å­˜åœ¨å¯èƒ½æ•…éšœæ¨¡å—ï¼Œå¹¶ç»™å‡ºæ¨¡å—å‡ºç°é¢‘æ¬¡
    - å¥åº·åº¦æ¿’ä¸´é˜ˆå€¼: {{'æ¬¡æ•°': int, 'æ—¶é—´èŒƒå›´': str, 'SOHèŒƒå›´': str}}ï¼Œè¡¨æ˜è®¾å¤‡å¥åº·åº¦æ¿’ä¸´é˜ˆå€¼
    å‹ç¼©æ—¥å¿—ä¿¡æ¯å¦‚ä¸‹ï¼š  
    {result}
    """
    return prompt


import openai
from dotenv import load_dotenv

def LLM_answer(message, api_key=None, base_url=None, model='gpt'):
    '''
    è°ƒç”¨å¤§æ¨¡å‹å›ç­”é—®é¢˜
    '''
    load_dotenv('.env')
    if model == 'gpt':
        if api_key is None:
            api_key = os.getenv('GPT_API_KEY')
            base_url = os.getenv('GPT_BASE_URL')
        client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        system_content = "ä½ æ˜¯ä¸€ä¸ªç”µå­è®¾å¤‡æ—¥å¿—åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®æ—¥å¿—ä¿¡æ¯ï¼Œç»™å‡ºæ‘˜è¦å’Œå»ºè®®æ“ä½œ"
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": message},
            ]
        )
        #print(response)
        content = response.choices[0].message.content
        if content is None:
            print(response.choices[0])
    return content