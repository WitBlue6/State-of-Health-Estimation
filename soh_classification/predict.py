import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # ä½¿ç”¨é•œåƒç½‘ç«™
from transformers import AutoModelForCausalLM, AutoTokenizer
#from fastapi import FastAPI
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import torch
import torch.nn as nn
from model import load_data, Standardization, set_random_seed
from utils import *
import re
import numpy as np
import matplotlib.pyplot as plt
import random


def generate_text(model, tokenizer, prompt: str, device="cpu"):
    '''è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹ç”Ÿæˆæ–‡æœ¬'''    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªç”µå­è®¾å¤‡æ—¥å¿—åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®æ—¥å¿—ä¿¡æ¯ï¼Œç»™å‡ºæœ€ç®€å•ç²¾ç‚¼æ—¥å¿—æ‘˜è¦ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„å»ºè®®æ‰§è¡Œæ“ä½œ"
    #full_prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    message = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]
    full_prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        inputs["input_ids"], 
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        do_sample=True,
        temperature=0.45,
        top_p=0.9,
        eos_token_id=tokenizer.eos_token_id,
    )
    gen_ids = outputs[0][inputs["input_ids"].shape[1]:]  # åªæ‹¿ç”Ÿæˆçš„æ–°token
    generated_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

    return generated_text


def data_process(data_path):
    """
    å¯¹æ•°æ®è¿›è¡ŒåŠ å™ªå£°å¹²æ‰°,å¾—åˆ°åŒ…å«å¼‚å¸¸æ•°æ®å’Œæ­£å¸¸æ•°æ®çš„åˆé›†
    :param data_path: å¤„ç†åçš„jsonè·¯å¾„
    :return: å¤„ç†åçš„prompts
    """
    # Load Dataset
    data_list = load_data(data_path, add_noise=False)
    dlen = len(data_list)
    # åŠ è½½å¼‚å¸¸æ•°æ®
    bad_path = os.path.join(os.path.dirname(data_path), 'æƒ¯ç»„Xè½´æ•…éšœ.txt')
    bad_data = load_data(bad_path, add_noise=False)
    nlen = dlen // 5
    nlen2 = 0
    for i in range(dlen):
        if i >= nlen and i < 3 * nlen:
            data_list[i] = bad_data[i - nlen]
            nlen2 += 1
    print(f'Successfully Transformed {len(bad_data)} Data to Bad Data!') 
    return data_list, nlen, nlen2



def model_detect(
        llm_path,
        soh_path, 
        cls_path,
        data_path, 
        threshold=0.05, 
        num_normal_samples=8,
        num_detect_samples=8, 
        num_modules=10,
        output_path='./outputs',
        normalize=True,
        filter=True,
        auto_threshold=False,
        seed=42
    ):
    """
    åŠ è½½æ¨¡å‹,å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹
    :param model_path: æ¨¡å‹è·¯å¾„
    :param data_list: è¦å¤„ç†çš„æ•°æ®åˆ—è¡¨
    :return: é¢„æµ‹ç»“æœ"""
    if seed is not None:
        set_random_seed(seed)
   
    # Get device
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        device = torch.device("mps")
        print("âœ… Using MPS (Apple Silicon)")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… Using CUDA (GPU {torch.cuda.get_device_name(0)})")
    else:
        device = torch.device("cpu")
        print("âš ï¸ Using CPU (no MPS or CUDA found)")
    dtype = torch.float32  # ä½¿ç”¨ä¸€è‡´çš„æ•°æ®ç±»å‹


    # åŠ è½½æ•°æ®
    print(f"Loading data from {data_path}...")
    data_list, nlen, nlen2 = data_process(data_path) # å‰nlenä¸ªæ ·æœ¬æ˜¯bad data
    
    # å¤„ç†æ•°æ®  å°†data_listä»å­—å…¸æ ¼å¼è½¬åŒ–ä¸ºnp.arrayï¼Œå¹¶ä½¿ç”¨np.vstackå°†å…¶å †å æˆä¸€ä¸ªäºŒç»´æ•°ç»„
    features = []
    for entry in data_list:
        feature_vector = np.array(list(entry.values()))
        features.append(feature_vector)
    features = np.vstack(features)
    print(f'Feature Shape:{features.shape}')
    
    # if normalize:
    #     print("Loading feature scaler...")
    #     scaler_path = os.path.join(os.path.dirname(soh_path), "feature_scalerwithoutLLM.pkl")
    #     import joblib
    #     feature_scaler, indices = joblib.load(scaler_path)
    #     #_, features = Standardization(features)
    #     for scaler, i in zip(feature_scaler, indices):
    #         features[:, i] = scaler.transform(features[:, i].reshape(-1, 1)).flatten()

    # åŠ è½½å¤§æ¨¡å‹
    torch.cuda.empty_cache()
    print(f"Loading LLM from {llm_path}...")
    tokenizer = AutoTokenizer.from_pretrained(llm_path, trust_remote_code=True)
    llm_model = AutoModelForCausalLM.from_pretrained(llm_path, trust_remote_code=True).to(device)

    print('Loading SOH Model...')
    # åŠ è½½SOHæ¨¡å‹æƒé‡
    checkpoint = torch.load(soh_path, map_location=device)
    soh_predictor = SOHPredictor(input_dim=features.shape[1]).to(device)
    soh_predictor.load_state_dict(checkpoint)

    checkpoint = torch.load(cls_path, map_location=device)
    cls_model = ClassificationModel(input_dim=features.shape[1], num_modules=num_modules).to(device)
    cls_model.load_state_dict(checkpoint)
    
    print(f"âœ…Loading Model Finished!")


    # æ£€æµ‹å¼‚å¸¸
    print(f"âš ï¸Detecting {len(features)} samples...")
    # å…ˆæ ¹æ®æ­£å¸¸å·¥ä½œæ—¶çš„æ ·æœ¬å¾—åˆ°æ­£å¸¸æ—¶çš„losså¤§å°
    normal_features = np.concatenate((features[:nlen], features[nlen + nlen2:]), axis=0)
    # ä»æ­£å¸¸æ ·æœ¬ä¸­éšæœºé€‰å–num_detect_samplesä¸ªæ ·æœ¬ä½œä¸ºæ­£å¸¸æ ·æœ¬
    inx = np.random.randint(0, nlen-num_normal_samples)
    #normal_features = normal_features[inx:inx + num_normal_samples]
    normal_features = normal_features[np.random.choice(len(normal_features), num_normal_samples, replace=False)]
    print(f'ğŸ˜‹Using Normal Samples: {len(normal_features)}')
    print(f'Normal Samples: {normal_features.shape}')
    # åŠ è½½SOHæ£€æµ‹å™¨
    from utils import SOHDetector
    soh_detector = SOHDetector(
            soh_predictor, 
            cls_model, 
            normal_features, 
            device, 
            threshold=threshold, 
            auto_threshold=auto_threshold, 
            normalize=normalize,
            sclar_soh_path='./outputs/scaler_soh.pkl',
            sclar_cls_path='./outputs/scaler_cls.pkl',
            filter=filter,
            print_log=False,
    )
    columns = [
        'Motor1', 'Motor2', 'Motor3', 'Motor4', 'Motor5', 'Motor6', 'Motor7', 'Motor8',
        'Motor9', 'Motor10', 'Motor11', 'Motor12', 'Motor13', 'Motor14', 'Motor15', 'Motor16',
        'Motor17', 'Motor18', 'Motor19', 'Motor20', 'Motor21', 'Motor22', 'Motor23', 'Motor24',
        'Accelx', 'Accely', 'Accelz', 'AngAcx', 'AngAcy', 'AngAcz', 'Eulerx', 'Eulery', 'Eulerz',
        'Voltage', 'Current', 'Power', 'Battery',
        'GPS_longitude', 'GPS_latitude', 'GPS_altitude'
    ]
    # æŒ‰num_detect_samplesä¸ªä¸€ç»„å¯¹æ ·æœ¬è¿›è¡ŒSOHé¢„æµ‹
    results = []
    thres_list = []
    warning_list = []
    import time
    start_time = time.time()
    for i in range(len(features)-num_detect_samples):
        batch_features = features[i:i+num_detect_samples]
        result, thres, warning_type = soh_detector.detect_soh(batch_features, key_map=columns)
        results.append(result)
        thres_list.append(thres)
        warning_list.append(warning_type)

    # ç»Ÿè®¡æ¨¡å—æ•…éšœæ•°é‡
    total = soh_detector.count[0] + soh_detector.count[1] + soh_detector.count[2] + soh_detector.count[3] + soh_detector.count[4] + soh_detector.count[5] + soh_detector.count[6] + soh_detector.count[7] + soh_detector.count[8]
    print(f'âœ…Detecting Finished, Total {len(results)} samples!')
    print(f'Error Counter')
    if total:
        print(f'Motor1:   {soh_detector.count[0]}  Motor1/Total: {soh_detector.count[0] / total * 100}%')
        print(f'Motor2:   {soh_detector.count[1]}  Motor2/Total: {soh_detector.count[1] / total * 100}%')
        print(f'Motor3:   {soh_detector.count[2]}  Motor3/Total: {soh_detector.count[2] / total * 100}%')
        print(f'Motor4:   {soh_detector.count[3]}  Motor4/Total: {soh_detector.count[3] / total * 100}%')
        print(f'EulerX:   {soh_detector.count[4]}  EulerX/Total: {soh_detector.count[4] / total * 100}%')
        print(f'EulerY:   {soh_detector.count[5]}  EulerY/Total: {soh_detector.count[5] / total * 100}%')
        print(f'EulerZ:   {soh_detector.count[6]}  EulerZ/Total: {soh_detector.count[6] / total * 100}%')
        print(f'Power:    {soh_detector.count[7]}  Power/Total: {soh_detector.count[7] / total * 100}%')
        print(f'Beidou:   {soh_detector.count[8]}  Beidou/Total: {soh_detector.count[8] / total * 100}%')
    # ä¿å­˜æ—¥å¿—ç»“æœ
    with open(os.path.join(output_path, 'log.txt'), 'w') as f:
        for log in soh_detector.log_info:
            f.write(log + '\n')
    #llm_output = LLM_answer(compress_logs(soh_detector.log_info), model='gpt')
    llm_output = generate_text(llm_model, tokenizer, prompt=compress_logs(soh_detector.log_info), device=device)
    print(f'Recommendation: \n{llm_output}')
    end_time = time.time()
    total_time = end_time - start_time
    print(f'â°Model Process Total Time: {total_time:3f}s')
    # è¾“å‡ºæ»¤æ³¢
    #if filter:
        #results = soh_filter(results, filter='gaussian', sigma=2.0)
    flags = [True if results[i] < (thres_list[i] * 100) else False for i in range(len(results))]
    # ç»˜å‡ºç»“æœå›¾
    plt.figure(figsize=(10, 6))
    plt.plot(results, color='b', label='SOH')
    plt.plot([thres * 100 for thres in thres_list], color='black', alpha=0.5, label='Threshold')
    plt.axvline(x=nlen-num_detect_samples, color='g', linestyle='--', label='Bad Data Start')
    plt.axvline(x=(nlen+nlen2), color='g', linestyle='-.', label='Bad Data End')
    anomaly_indices = [i for i, flag in enumerate(flags) if flag]
    # é¢„è­¦ç±»å‹è¾“å‡º
    # æ•…éšœé¢„è­¦
    if anomaly_indices:
        plt.scatter(
            anomaly_indices, 
            [results[i] for i in anomaly_indices],
            color='red', marker='x', label='Detected Anomalies'
        )
    # ä¸‹æ»‘é¢„è­¦
    decrease_flags = [True if warning == 2 else False for warning in warning_list]
    decrease_indices = [i for i, flag in enumerate(decrease_flags) if flag]
    if decrease_indices:
        plt.scatter(
            decrease_indices,
            [results[i] for i in decrease_indices],
            color='orange', marker='^', label='Decrease Warning'
        )
    # é•¿æœŸä¸´ç•Œé¢„è­¦
    critical_flags = [True if warning == 3 else False for warning in warning_list]
    critical_indices = [i for i, flag in enumerate(critical_flags) if flag]
    if critical_indices:
        plt.scatter(
            critical_indices,
            [results[i] for i in critical_indices],
            color='green', marker='*', label='Long-Term Critical Warning'
        )
    plt.xlabel('Batch Index')
    plt.ylabel('SOH Value')
    plt.title('SOH Prediction')
    plt.ylim([0, 102])
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(output_path, 'soh_predictionwithoutLLM.png'))
    plt.show()
    print('ğŸ“ŠResults has been saved')

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument("--llm_path", type=str, default='Qwen/Qwen1.5-1.8B-Chat')
    parser.add_argument("--soh_path", type=str, default='./outputs/best_sohmodel.pth')
    parser.add_argument("--cls_path", type=str, default='./outputs/best_classification.pth')
    parser.add_argument("--data_path", type=str, default='./dataset/æ— å¼‚å¸¸.txt')
    parser.add_argument("--threshold", type=float, default=0.86)
    parser.add_argument("--num_normal_samples", type=int, default=32, help="å¤šå°‘ä¸ªæ­£å¸¸æ ·æœ¬ç”¨äºæ±‚è§£æ­£å¸¸æ—¶çš„loss")
    parser.add_argument("--num_detect_samples", type=int, default=32, help="ä»¥å¤šå°‘ä¸ªæ ·æœ¬ä¸ºä¸€ç»„è¿›è¡Œé¢„æµ‹ï¼Œæé«˜é²æ£’æ€§")
    parser.add_argument("--num_modules", type=int, default=10, help="æ¨¡å—æ•°")
    parser.add_argument("--output_path", type=str, default="./outputs")
    parser.add_argument("--normalize", type=bool, default=True)
    parser.add_argument("--filter", type=bool, default=True, help="æ˜¯å¦è¿›è¡Œè¾“å‡ºæ»¤æ³¢")
    parser.add_argument("--auto_threshold", type=bool, default=True, help="è‡ªé€‚åº”é˜ˆå€¼")
    parser.add_argument("--seed", type=int, default=42)  #999

    args = parser.parse_args()
    model_detect(**vars(args))