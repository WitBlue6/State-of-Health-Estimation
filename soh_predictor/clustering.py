from sklearn.cluster import KMeans
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import os
from model import load_prompts, generate_features, Standardization
import re

def extract_features(prompt):
    """
    ä»promptä¸­æå–é”®å€¼å¯¹
    :param prompt: è¾“å…¥çš„promptå­—ç¬¦ä¸²
    :return: æå–çš„é”®å€¼å¯¹å­—å…¸
    """
    pattern = r"([\w\s]+):([0-9\.]+)"
    matches = re.findall(pattern, prompt)
    feature_dict = {key.strip(): float(value) for key, value in matches}
    return feature_dict

def data_process(data_path):
    """
    å¯¹æ•°æ®è¿›è¡ŒåŠ å™ªå£°å¹²æ‰°,å¾—åˆ°åŒ…å«å¼‚å¸¸æ•°æ®å’Œæ­£å¸¸æ•°æ®çš„åˆé›†
    :param data_path: å¤„ç†åçš„jsonè·¯å¾„
    :return: å¤„ç†åçš„prompts
    """
    # Load Dataset
    prompts = load_prompts(data_path)
    data_list = list(map(extract_features, prompts))  # å¾—åˆ°äº†åŸå§‹æ•°æ®å­—å…¸åˆ—è¡¨
    # å¯¹æ•°æ®è¿›è¡Œå¤„ç†,åŠ ç™½å™ªå£°å¹²æ‰°
    dlen = len(data_list)  # print(dlen) ## 2080
    nlen = dlen // 3  # å–1/3çš„æ•°æ®ä½œä¸ºå™ªå£°æ•°æ®  ##  693
    bad_data = []
    init_data = []
    # å¾—åˆ°æ•°æ®çš„å¹³å‡å¹…åº¦å€¼
    tim_voltage = [d['timer voltage'] for d in data_list]
    tim_current = [d['timer current'] for d in data_list]
    tim_temperature = [d['timer temperature'] for d in data_list]
    voltage = sum(tim_voltage) / len(tim_voltage)
    current = sum(tim_current) / len(tim_current)
    temperature = sum(tim_temperature) / len(tim_temperature)
    # åŠ å™ªå£°å¤„ç†
    for i in range(dlen):  
        if i < nlen:
            for key in data_list[i]:
                if key in ['timer voltage']: #, 'power voltage', 'control voltage', 'dsp voltage'
                    data_list[i][key] += np.random.normal(0, 0.005*voltage)  # åŠ ç™½å™ªå£°
                elif key in ['timer current']: #, 'power current', 'control current', 'dsp current'
                    data_list[i][key] += np.random.normal(0, 0.005*current)  # åŠ ç™½å™ªå£°
                elif key in ['timer temperature']: # , 'power temperature', 'control temperature', 'dsp temperature'
                    data_list[i][key] += np.random.normal(0, 0.005*temperature)  # åŠ ç™½å™ªå£°
            bad_data.append(data_list[i])
        else:
            init_data.append(data_list[i])
    print(f'Successfully Transformed {nlen} Data to Bad Data!') 
    data_list = bad_data + init_data
    def generate_full_prompt(example):
        full_prompt = f"""
                ### Instruction:
                {example["instruction"]}
                ### Input:
                {example["input"]}
			"""
        return full_prompt
    def generate_prompt(examples):
        prompts = []
        for row in examples:
            instruction = f"You are a data feature extraction expert. Using the provided data, extract the features."
            input_text = (
                f"timer voltage:{row['timer voltage']}, "
                f"power voltage:{row['power voltage']}, "
                f"control voltage:{row['control voltage']}, "
                f"dsp voltage:{row['dsp voltage']}, "
                f"timer current:{row['timer current']}, "
                f"power current:{row['power current']}, "
                f"control current:{row['control current']}, "
                f"dsp current:{row['dsp current']}, "
                f"timer temperature:{row['timer temperature']}, "
                f"power temperature:{row['power temperature']}, "
                f"control temperature:{row['control temperature']}, "
                f"dsp temperature:{row['dsp temperature']},"
            )
            prompts.append({
                "instruction": instruction,
                "input": input_text,
            })
        return prompts
    prompts = generate_prompt(data_list)
    prompts = list(map(generate_full_prompt, prompts))
    return prompts, len(bad_data)


def calculate_soh(features, center, noraml, threshold=0.8):
    """
    æ ¹æ®æ¯ä¸ªæ ·æœ¬ä¸èšç±»ä¸­å¿ƒçš„è·ç¦»è®¡ç®—SOH
    :param features: è¾“å…¥çš„ç‰¹å¾æ•°æ®
    :param center: èšç±»ä¸­å¿ƒ
    :param threshold: åˆ¤æ–­å¼‚å¸¸çš„é˜ˆå€¼
    :return: SOHå€¼,å¼‚å¸¸æ ‡ç­¾
    """
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬åˆ°èšç±»ä¸­å¿ƒçš„æ¬§æ°è·ç¦»
    distances = np.linalg.norm(features - center, axis=1)
    mean_d = np.mean(distances)
    # å½’ä¸€åŒ–è·ç¦»ï¼Œè·ç¦»è¶Šå¤§ï¼ŒSOHå€¼è¶Šä½
    d = (mean_d - noraml[0]) / noraml[1]
    soh = torch.clamp(torch.tensor((1 - d) * 100), min=0, max=100).item()
    # åˆ¤æ–­å¼‚å¸¸ï¼šè·ç¦»è¶…è¿‡é˜ˆå€¼çš„ç‚¹è®¤ä¸ºæ˜¯å¼‚å¸¸
    flags = soh < threshold * 100  
    return soh, flags

def model_detect(
        llm_path, 
        soh_path, 
        data_path, 
        threshold=0.8, 
        num_normal_samples=8,
        num_detect_samples=8, 
        output_path='./outputs',
        normalize=True,
        clustering_method='kmeans'  # èšç±»æ–¹æ³•é€‰æ‹©
    ):
    """
    åŠ è½½æ¨¡å‹,å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹
    :param model_path: æ¨¡å‹è·¯å¾„
    :param data_list: è¦å¤„ç†çš„æ•°æ®åˆ—è¡¨
    :return: é¢„æµ‹ç»“æœ"""
    # Get device
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        device = torch.device("mps")
        print("âœ… Using MPS (Apple Silicon)")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… Using CUDA (GPU {torch.cuda.get_device_name(0)})")
    else:
        device = torch.device("cpu")
        print("âš ï¸ Using CPU (no MPS or CUDA found)")
    dtype = torch.float32  # ä½¿ç”¨ä¸€è‡´çš„æ•°æ®ç±»å‹

    # åŠ è½½æ•°æ®
    print(f"Loading data from {data_path}...")
    prompts, nlen = data_process(data_path) # å‰nlenä¸ªæ ·æœ¬æ˜¯bad data
    # LLMè·å–ç‰¹å¾
    model = AutoModelForCausalLM.from_pretrained(llm_path).to(device)
    tokenizer = AutoTokenizer.from_pretrained(llm_path)
    print(f"Generating features for {len(prompts)} samples...")
    features = generate_features(model, tokenizer, prompts, device)
    if normalize:
        print('Normalizing Features...')
        features = Standardization(features) 
    
    # ä»æ­£å¸¸æ ·æœ¬ä¸­é€‰å–ä¸€äº›æ¥è®¡ç®—èšç±»ä¸­å¿ƒ
    normal_features = features[nlen:]  # è¿™é‡Œå‡è®¾åé¢çš„æ ·æœ¬æ˜¯æ­£å¸¸çš„
    normal_features = normal_features[np.random.choice(len(normal_features), num_normal_samples, replace=False)]
    
    # ä½¿ç”¨ K-means èšç±»æ–¹æ³•æ¥è®¡ç®—èšç±»ä¸­å¿ƒ
    kmeans = KMeans(n_clusters=1, random_state=42)
    kmeans.fit(normal_features)  # è·å–èšç±»ä¸­å¿ƒ
    cluster_center = kmeans.cluster_centers_[0]  # è·å–èšç±»ä¸­å¿ƒ

    # å…ˆè®¡ç®—num_normal_samplesä¸ªæ ·æœ¬çš„å¹³å‡è·ç¦»
    distances = np.linalg.norm(normal_features - cluster_center, axis=1)
    avg_distance = np.mean(distances)
    std_distance = np.std(distances)
    print(f'Average Distance: {avg_distance}, Standard Deviation: {std_distance}')

    # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„SOH
    print(f"Calculating SOH for {len(features)} samples...")
    results = []
    flags = []
    for i in range(0, len(features), num_detect_samples):
        batch_features = features[i:i+num_detect_samples]
        result, flag = calculate_soh(batch_features, cluster_center, [avg_distance, std_distance], threshold)
        results.append(result)
        flags.append(flag)

    # ç»˜åˆ¶ SOH æ›²çº¿
    plt.figure(figsize=(12, 8))
    plt.plot(results, label='SOH')
    plt.axhline(y=threshold * 100, color='r', linestyle='--', label='Threshold')
    plt.axvline(x=nlen // num_detect_samples, color='g', linestyle='--', label='Bad Data')
    plt.xlabel('Batch Index')
    plt.ylabel('SOH Value')
    plt.title('SOH Prediction based on Clustering')
    #plt.ylim([40, 103])
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(output_path, 'soh_prediction_distance.png'))
    plt.show()
    print('ğŸ“ŠResults has been saved')

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument("--llm_path", type=str, default='EleutherAI/pythia-160m')
    parser.add_argument("--soh_path", type=str, default='./outputs/soh_predictor.pth')
    parser.add_argument("--data_path", type=str, default='./dataset/1533B.json')
    parser.add_argument("--threshold", type=float, default=0.8)
    parser.add_argument("--num_normal_samples", type=int, default=16, help="å¤šå°‘ä¸ªæ­£å¸¸æ ·æœ¬ç”¨äºè®¡ç®—èšç±»ä¸­å¿ƒ")
    parser.add_argument("--num_detect_samples", type=int, default=8, help="ä»¥å¤šå°‘ä¸ªæ ·æœ¬ä¸ºä¸€ç»„è¿›è¡Œé¢„æµ‹ï¼Œæé«˜é²æ£’æ€§")
    parser.add_argument("--output_path", type=str, default="./outputs")
    parser.add_argument("--normalize", type=bool, default=True)

    args = parser.parse_args()
    model_detect(**vars(args))
