import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # ä½¿ç”¨é•œåƒç½‘ç«™
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from datasets import load_dataset
import torch
import torch.nn as nn
from model import load_prompts, generate_features, Standardization, set_random_seed
from utils import SOHPredictor, AnomalyProcessor
import re
import numpy as np
import matplotlib.pyplot as plt
import random

def extract_features(prompt):
    """
    ä»promptä¸­æå–é”®å€¼å¯¹
    :param prompt: è¾“å…¥çš„promptå­—ç¬¦ä¸²
    :return: æå–çš„é”®å€¼å¯¹å­—å…¸
    """
    pattern = r"([\w\s]+):([0-9\.]+)"
    matches = re.findall(pattern, prompt)
    feature_dict = {key.strip(): float(value) for key, value in matches}
    return feature_dict

def data_process(data_path):
    """
    å¯¹æ•°æ®è¿›è¡ŒåŠ å™ªå£°å¹²æ‰°,å¾—åˆ°åŒ…å«å¼‚å¸¸æ•°æ®å’Œæ­£å¸¸æ•°æ®çš„åˆé›†
    :param data_path: å¤„ç†åçš„jsonè·¯å¾„
    :return: å¤„ç†åçš„prompts
    """
    # Load Dataset
    prompts = load_prompts(data_path)
    data_list = list(map(extract_features, prompts))  # å¾—åˆ°äº†åŸå§‹æ•°æ®å­—å…¸åˆ—è¡¨
    # å¯¹æ•°æ®è¿›è¡Œå¤„ç†,åŠ ç™½å™ªå£°å¹²æ‰°
    dlen = len(data_list)  # print(dlen) ## 2080
    nlen = dlen // 5  # å–1/3çš„æ•°æ®ä½œä¸ºå™ªå£°æ•°æ®  ##  693
    bad_data = []
    # å¾—åˆ°æ•°æ®çš„å¹³å‡å¹…åº¦å€¼
    tim_voltage = [d['timer voltage'] for d in data_list]
    tim_current = [d['timer current'] for d in data_list]
    tim_temperature = [d['timer temperature'] for d in data_list]
    voltage = 0.3 * (sum(tim_voltage) / len(tim_voltage))
    current = 0.3 * (sum(tim_current) / len(tim_current))
    temperature = 0.3 * (sum(tim_temperature) / len(tim_temperature))
    print(voltage, current, temperature)
    # å¯¹1/3*dlenåˆ°2/3*dlençš„æ•°æ®è¿›è¡ŒåŠ ç™½å™ªå£°å¹²æ‰°
    key_noise_map = {
        #'timer voltage': voltage,
        #'timer current': current,
        #'timer temperature': temperature,
        'power voltage': voltage,
        'power current': current,
        'power temperature': temperature,
        #'control voltage': voltage,
        #'control current': current,
        #'control temperature': temperature,
        #'dsp voltage': voltage,
        #'dsp current': current, 
        #'dsp temperature': temperature,
    }

    # for i in range(dlen): 
    #     if i >= nlen and i < 2 * nlen:
    #         available_keys = [key for key in data_list[i].keys() if key in key_noise_map]
    #         if not available_keys:
    #             continue
    #         num_keys_to_select = random.randint(1, len(available_keys)//3)
    #         selected_keys = random.sample(available_keys, num_keys_to_select)
    #         #selected_keys = available_keys
    #         for key in selected_keys:
    #             data_list[i][key] += np.random.normal(0, key_noise_map[key])
    #         bad_data.append(data_list[i])
    process = AnomalyProcessor()
    for i in range(dlen): 
        if i >= nlen and i < 2 * nlen:
            #data_list[i] = process.missing_adder([data_list[i]], missing_rate=0.001)[0]
            #data_list[i] = process.outlier_adder([data_list[i]], outlier_rate=0.001, factor=2.0)[0]
            data_list[i] = process.data_random_process([data_list[i]], noise_level=0.15, rate=0.01, factor=2.0)[0]
            bad_data.append(data_list[i])
    print(f'Successfully Transformed {nlen} Data to Bad Data!') 
    return data_list, nlen, len(bad_data)


def soh_filter(data, filter='ma', window_size=5, alpha=0.2, sigma=1.0):
    """
    å¯¹è¾“å‡ºSOHè¿›è¡Œæ»¤æ³¢,æé«˜æ¨¡å‹é²æ£’æ€§
    """
    if filter == 'ma':  #ç§»åŠ¨å¹³å‡æ»¤æ³¢
        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')
    elif filter == 'ewma':  #æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡æ»¤æ³¢
        return np.convolve(data, np.array([alpha**i for i in range(window_size)]), mode='same')
    # é«˜æ–¯æ»¤æ³¢
    elif filter == 'gaussian':
        print('Using Gaussian Filter')
        from scipy.ndimage import gaussian_filter1d
        return gaussian_filter1d(data, sigma=sigma)
    else:
        print(f'â€¼ï¸Warning: Unknown filter type {filter}, using no filter!!!')
        return data


def model_detect(
        soh_path, 
        data_path, 
        threshold=0.05, 
        num_normal_samples=8,
        num_detect_samples=8, 
        output_path='./outputs',
        normalize=True,
        filter=True,
        auto_threshold=False,
        seed=42
    ):
    """
    åŠ è½½æ¨¡å‹,å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹
    :param model_path: æ¨¡å‹è·¯å¾„
    :param data_list: è¦å¤„ç†çš„æ•°æ®åˆ—è¡¨
    :return: é¢„æµ‹ç»“æœ"""
    if seed is not None:
        set_random_seed(seed)
    # Get device
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        device = torch.device("mps")
        print("âœ… Using MPS (Apple Silicon)")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… Using CUDA (GPU {torch.cuda.get_device_name(0)})")
    else:
        device = torch.device("cpu")
        print("âš ï¸ Using CPU (no MPS or CUDA found)")
    dtype = torch.float32  # ä½¿ç”¨ä¸€è‡´çš„æ•°æ®ç±»å‹

    # åŠ è½½æ•°æ®
    print(f"Loading data from {data_path}...")
    data_list, nlen, nlen2 = data_process(data_path) # å‰nlenä¸ªæ ·æœ¬æ˜¯bad data
    
    # å¤„ç†æ•°æ®  å°†data_listä»å­—å…¸æ ¼å¼è½¬åŒ–ä¸ºnp.arrayï¼Œå¹¶ä½¿ç”¨np.vstackå°†å…¶å †å æˆä¸€ä¸ªäºŒç»´æ•°ç»„
    features = []
    for entry in data_list:
        feature_vector = np.array(list(entry.values()))
        features.append(feature_vector)
    features = np.vstack(features)
    print(f'Feature Shape:{features.shape}')
    
    if normalize:
        print("Loading feature scaler...")
        scaler_path = os.path.join(os.path.dirname(soh_path), "feature_scalerwithoutLLM.pkl")
        import joblib
        feature_scaler = joblib.load(scaler_path)
        features = feature_scaler.transform(features)


    # åŠ è½½SOHæ¨¡å‹æƒé‡
    checkpoint = torch.load(soh_path, map_location=device)
    soh_predictor = SOHPredictor(input_dim=features.shape[1]).to(device)
    soh_predictor.load_state_dict(checkpoint)

    # æ£€æµ‹å¼‚å¸¸
    print(f"âš ï¸Detecting {len(features)} samples...")
    # å…ˆæ ¹æ®æ­£å¸¸å·¥ä½œæ—¶çš„æ ·æœ¬å¾—åˆ°æ­£å¸¸æ—¶çš„losså¤§å°
    normal_features = np.concatenate((features[:nlen], features[nlen + nlen2:]), axis=0)
    # ä»æ­£å¸¸æ ·æœ¬ä¸­éšæœºé€‰å–num_detect_samplesä¸ªæ ·æœ¬ä½œä¸ºæ­£å¸¸æ ·æœ¬
    normal_features = normal_features[np.random.choice(len(normal_features), num_normal_samples, replace=False)]
    print(f'ğŸ˜‹Using Normal Samples: {len(normal_features)}')

    # åŠ è½½SOHæ£€æµ‹å™¨
    from utils import SOHDetector
    soh_detector = SOHDetector(soh_predictor, normal_features, device, threshold=threshold, auto_threshold=auto_threshold)
    
    # æŒ‰num_detect_samplesä¸ªä¸€ç»„å¯¹æ ·æœ¬è¿›è¡ŒSOHé¢„æµ‹
    results = []
    thres_list = []
    for i in range(num_detect_samples, len(features)):
        batch_features = features[i-num_detect_samples:i]
        result, thres = soh_detector.detect_soh(batch_features)
        results.append(result)
        thres_list.append(thres)
    # è¾“å‡ºæ»¤æ³¢
    if filter:
        results = soh_filter(results, filter='gaussian', sigma=5.0)
    flags = [True if results[i] < (thres_list[i] * 100) else False for i in range(len(results))]
    # ç»˜å‡ºç»“æœå›¾
    plt.figure(figsize=(6, 4))
    plt.plot(results, color='b', label='SOH')
    plt.plot([thres * 100 for thres in thres_list], color='black', alpha=0.5, label='Threshold')
    plt.axvline(x=nlen, color='g', linestyle='--', label='Bad Data Start')
    plt.axvline(x=(nlen+nlen2), color='g', linestyle='-.', label='Bad Data End')
    anomaly_indices = [i for i, flag in enumerate(flags) if flag]
    if anomaly_indices:
        plt.scatter(
            anomaly_indices, 
            [results[i] for i in anomaly_indices],
            color='red', marker='x', label='Detected Anomalies'
        )
    plt.xlabel('Batch Index')
    plt.ylabel('SOH Value')
    plt.title('SOH Prediction')
    plt.ylim([0, 102])
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(output_path, 'soh_predictionwithoutLLM.png'))
    plt.show()
    print('ğŸ“ŠResults has been saved')

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()

    parser.add_argument("--soh_path", type=str, default='./outputs/best_modelwithoutLLM.pth')
    parser.add_argument("--data_path", type=str, default='./dataset/1533B.json')
    parser.add_argument("--threshold", type=float, default=0.8)
    parser.add_argument("--num_normal_samples", type=int, default=32, help="å¤šå°‘ä¸ªæ­£å¸¸æ ·æœ¬ç”¨äºæ±‚è§£æ­£å¸¸æ—¶çš„loss")
    parser.add_argument("--num_detect_samples", type=int, default=32, help="ä»¥å¤šå°‘ä¸ªæ ·æœ¬ä¸ºä¸€ç»„è¿›è¡Œé¢„æµ‹ï¼Œæé«˜é²æ£’æ€§")
    parser.add_argument("--output_path", type=str, default="./outputs")
    parser.add_argument("--normalize", type=bool, default=True)
    parser.add_argument("--filter", type=bool, default=True, help="æ˜¯å¦è¿›è¡Œè¾“å‡ºæ»¤æ³¢")
    parser.add_argument("--auto_threshold", type=bool, default=True, help="è‡ªé€‚åº”é˜ˆå€¼")
    parser.add_argument("--seed", type=int, default=42)  #999

    args = parser.parse_args()
    model_detect(**vars(args))